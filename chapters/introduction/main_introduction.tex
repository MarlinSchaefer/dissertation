\chapter{Introduction}
The Universe is a vast space holding many secrets. Each new observational channel and instrument has brought with it new discoveries and peeled back the curtain on fundamental physics further. With the advent of telescopes humanity discovered more intricate details about the solar system~\cite{Herschel:2012aaa}. Radio telescopes led to the discovery of the cosmic microwave background~\cite{Penzias:1965aaa}. Infrared telescopes, like the recently launched James Webb Space Telescope~\cite{Gardner:2006ky}, allow us to study the distant past of the Universe. X-ray observations have improved the understanding of stars and especially supernovae~\cite{Hughes:2000aaa}. Gamma ray observations unveiled a new type of signal known as gamma ray bursts~\cite{Klebesadel:1973aaa}, the sources of which have been a mystery for a long time.

The newest tool in the pocket of astronomers are kilometer scale laser interferometric gravitational-wave (\acrshort{gw}) detectors~\cite{LIGOScientific:2014pky, VIRGO:2014yos, KAGRA:2018plz, Luck:2010rt}. They allow to probe the Universe using gravity as a messenger medium, which opens up a completely new view into the cosmos. \acrshort{gw}s were first hypothesized to exist by Einstein as a direct consequence of his theory of general relativity (\acrshort{gr})~\cite{Einstein:1916aaa} but were believed to be too weak to ever be observed directly. Although various attempts were made~\cite{Weber:1960zz} and indirect evidence of their existence was found in the 1980s~\cite{Taylor:1982zz}, a direct detection took almost $100$ years after their initial theoretical description. On September 14th, 2015 the advanced laser interferometer gravitational-wave observatory (\acrshort{ligo}) detectors picked up the first confirmed direct detection of a \acrshort{gw}~\cite{LIGOScientific:2016aoc}. The signal was emitted from a merging system of two black holes (\acrshort{bh}s).

In the $7$ years following the first detection new observations have become a common occurrence and almost $100$ sources have been confirmed to date~\cite{LIGOScientific:2021djp, Nitz:2021zwj}. This plethora of signals has allowed for new tests of fundamental physics~\cite{LIGOScientific:2016lio, LIGOScientific:2021sio, LIGOScientific:2018cki, Capano:2019eae, LIGOScientific:2018gmd, DES:2019ccw} and insights into the contents of the Universe~\cite{LIGOScientific:2021psn, LIGOScientific:2021job}. In 2017 the first binary neutron star (\acrshort{bns}) merger was observed~\cite{LIGOScientific:2017vwq}. It was accompanied by an electromagnetic counterpart~\cite{Coulter:2017wya, DES:2017kbs, LIGOScientific:2017ync}, first picked up as a gamma ray burst, thus confirming that at least some gamma ray bursts originate from \acrshort{bns} mergers. In 2019 two signals from binary systems whose component masses are consistent with neutron star\textendash{}black hole binaries (\acrshort{nsbh})s were detected~\cite{LIGOScientific:2021qlt}, completing the set of expected detectable compact binaries.

To detect these signals, many technological breakthroughs both on the instrumental and data analysis side were required~\cite{LIGOScientific:2022aaa, LIGOScientific:2014pky, Allen:2005fk, Buonanno:2000ef, Ajith:2007qp}. The \SI{4}{\kilo\metre} long arms of the interferometers change their length by \SI[parse-numbers=false]{\approx 10^{-18}}{\metre}~\cite{LIGOScientific:2016aoc, LIGOScientific:2022aab}. Consequently, a lot of noise sources can have a stronger effect on the detector readout than \acrshort{gw}s~\cite{LIGOScientific:2014pky}. These noise sources need to be supressed as much as possible by the instrument design. However, some fundamental limits~\cite{Heurs:2018wsu} exist and, therefore, \acrshort{gw} signals are not clearly visible in the detector output. Extracting weak signals from the noise floor requires sophisticated data analysis methods. While instrumental development is paramount to \acrshort{gw} detection, this thesis touches only briefly on challenges in this field and focuses mainly on analyzing the resulting data.

Today the most sensitive data analysis methods to detect \acrshort{gw}s from compact binary mergers rely on a process known as matched filtering~\cite{Wiener:1949aaa, Allen:2005fk}. Matched filtering is the optimal discriminator between stationary Gaussian noise that contains a known signal and pure stationary Gaussian noise~\cite{Allen:2005fk}. It compares the known signal with the data and checks how well the two match. There also exist search methods that only loosely model the source and are, therefore, more sensitive to signals of unknown shape and origin~\cite{ligo_pipelines, Klimenko:2005xv, Klimenko:2015ypf, Lynch:2015yin}.

Matched filtering assumes that the signal in the data is known exactly which is not true in a realistic search scenario. As a consequence, one needs to filter for a whole set of possible signals. This set is known as the template bank and it discretizes the continuous parameter space of potential sources. The computational cost of a matched filter search scales linearly with the number of templates in the template bank, as the data has to be filtered against each template individually. However, the size of the template bank scales exponentially with the number of parameters used to describe the sources. Therefore, current searches make some simplifying assumptions that reduce the dimensionality of the parameter space but also limit the modeled physics. The discrete nature of the template bank and the simplifications used to construct it can cause some signals to be missed~\cite{Harry:2016ijz, Harry:2017weg, CalderonBustillo:2016rlt, Chandra:2022ixv, Dhurkunde:2022aek}. Additionally, the number of templates has to be increased as detector sensitivity in the future is expected to improve at low frequencies relative to high frequencies~\cite{KAGRA:2018plz, LIGOScientific:2017aaa, Maggiore:2019uih, Reitze:2019iox}. With this relative increase in sensitivity, the early part of the signal is weighted more strongly and discrepancies between signal and template accumulate over time, thus requiring a denser coverage of the parameter space. To reduce the computational burden, include more physically relevant effects, or increase the parameter space which we are sensitive to, more efficient search algorithms are desirable.

Machine learning (\acrshort{ml}) methods are one possible avenue to reduce computational costs that have started to be explored in the last decade~\cite{Cuoco:2020ogp}. Especially methods based on deep learning have recently gathered significant interest. Deep learning is a field of computer science where artificial neurons are connected to networks and collectively trained to solve a given task. It has been successfully applied to numerous other problems and is often computationally more efficient than alternative methods~\cite{krizhevsky:2012, Ren:2015aaa, openai:2019}. The hope is that \acrshort{ml} can generalize to unseen regions of parameter space and new signals without increasing the computational burden too much. Furthermore, they may be capable of improving on matched filtering based searches when the detector noise is not stationary or Gaussian. They could also outperform matched filter searches due to the discreteness of the template banks~\cite{Prix:2009tq, Baker:2014eba, Yan:2021wml}.

Today most of these goals are still out of reach for current algorithms and works are often at a proof of principle level. Many studies target limited parameter spaces that are efficiently searched by existing search algorithms~\cite{George:2016hay, Gabbard:2017lja, Wei:2020ztw, Yan:2021wml}. \acrshort{ml} search algorithms for long duration low mass \acrshort{bns} or \acrshort{nsbh} signals, where current searches are expensive to run, are still rarely explored due to problems with processing large inputs. Furthermore, studies often use Gaussian noise for training and evaluation. Consequently, it is difficult to judge the real world applicability of these algorithms. This is further complicated by the common usage of metrics which are inspired by deep learning literature rather than existing metrics developed for traditional \acrshort{gw} searches.

The work discussed in this thesis aims at pushing deep learning searches beyond the proof of principle stage. All studies presented here estimate sensitive distances, a metric commonly used for state-of-the-art \acrshort{gw} detection pipelines, at previously untested false-alarm rates (\acrshort{far}s). To enable these tests, several new methods are developed which significantly improve the performance over other machine learning search algorithms at astrophysically relevant \acrshort{far}s. This work allows for a clear comparison between advanced deep learning \acrshort{gw} search algorithms and existing production search pipelines. Such comparisons had previously been difficult, due to differing metrics. From these, it is inferred that the presented solutions are already competitive for some parameter regions. For other regions, where machine learning searches are currently still outperformed by traditional algorithms, our analyses reveal the main problems that need to be addressed to elevate these novel algorithms to state-of-the-art performance.

More specifically, chapter \ref{ch:bns} presents a new deep learning model designed for \acrshort{bns} detection. It performs significantly better at low \acrshort{far}s than previous deep learning searches but also finds a gap in performance compared to PyCBC Live~\cite{Nitz:2018rgo}, a state-of-the-art low-latency production search pipeline. Chapter \ref{ch:training_strats} re-analyzes an existing deep learning algorithm and introduces a modification to prevent a collapse of the sensitivity at low \acrshort{far}s. It also investigates the impact of different training strategies on the final performance of the network. Chapter \ref{ch:cnn_coinc} adapts the coincidence analysis used in production searches~\cite{Messick:2016aqy, Usman:2015kfa, Klimenko:2015ypf} to a deep learning search algorithm trained for a single detector. This trivial extension of the deep learning search algorithm reduces the \acrshort{far}s at which it can be tested by several orders of magnitude at a negligible computational cost. Chapter \ref{ch:mlgwsc1} presents the results of a mock data challenge led by myself that makes the tools and experiences gained in previous studies available to the global community. It compares several contributions from international groups to production level searches and assesses the current state of the field.
