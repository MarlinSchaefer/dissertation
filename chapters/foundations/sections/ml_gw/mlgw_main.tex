\section{Machine Learning in Gravitational-Wave Astronomy}\label{sec:ml-gw-hist}
Machine learning is a computational tool that has started to gain renewed interest in the early 2010s. It is only natural for scientists to evaluate the usefulness of new tools to their own area of research. However, there are reasons beyond the academic curiousness that justify a thorough investigation of the capability of modern \acrshort{ml} algorithms to solve some of the problems in \acrshort{gw} data analysis. As previously discussed, since the first observation of a \acrshort{gw} the rate of detections has rapidly increased~\cite{Nitz:2021zwj} and is expected to grow faster as detectors are upgraded~\cite{KAGRA:2013rdx, Cahillane:2022pqm}. This necessitates the use of highly optimized data analysis algorithms to process the data in real time and produce accurate and reliable alerts. Additionally, it is desirable to produce accurate sky-maps of the expected origin of each signal to allow for prompt \acrshort{em} follow-up and to be able to extract more information from some mergers~\cite{Nitz:2020vym}. \acrshort{ml} algorithms are known for their capability to discover patterns in data and their computational efficiency has enabled many advances in other scientific fields such as computer vision~\cite{Goodfellow:2016:DNN, krizhevsky:2012}. This makes them a great contender to solve many of the above mentioned problems.

Many applications of \acrshort{ml} to \acrshort{gw} data analysis have already been studied. These include the identification and classification of glitches in the detector output, partial and full \acrshort{gw} search pipelines, and parameter estimation algorithms. Below I will give an overview of recent developments in the field and will mainly focus on algorithms relevant to \acrshort{cbc} signals. A great and more general overview of the field can be found in \cite{Cuoco:2020ogp}.

\subsection{Data Quality}
One of the first \acrshort{gw} research fields that has made use of \acrshort{ml} algorithms is the classification of data quality of the detector output~\cite{Mukherjee:2010zza, Biswas:2013wfa, Powell:2015ona, Powell:2016rkl}. As previously discussed, the detector output contains many non-Gaussian noise transients known as glitches. Finding and classifying these glitches is important to identify their source and to reduce the number of false alarms. \acrshort{ml} can be useful to identify different categories of glitches or predict them from recordings of external sensors that monitor noise sources, such as ground motion or \acrshort{em} interference~\cite{Effler:2015aaa}, which couple highly non-linearly into the detector output.

The citizen science project GravitySpy~\cite{Zevin:2016qwy} was started in 2016 and asks volunteers to classify time-frequency representations of different glitches. It leverages the Zooniverse platform and combines the use of \acrshort{ml} algorithms with human categorization to classify glitches into known and unknown categories. The resulting data sets are then used to train machine learning models~\cite{Bahaadini:2018git}. Importantly, the input data is a direct product of the detector output and does not take into account auxiliary data channels.

A different approach is taken by the iDQ pipeline, which tries to predict the presence of glitches only from auxiliary sensor data in real time~\cite{Essick:2020qpo}. It monitors $\mathcal{O}\lr{10^3}$ auxiliary channels to produce a classification into ``glitch'' or ``no-glitch'' for every time step. The underlying \acrshort{ml} algorithms are continually updated to account for non-stationarity of the noise. iDQ has been in use throughout the first three observing runs~\cite{LIGOScientific:2021djp} and contributed to the rapid release of GW170817, which coincided with a glitch~\cite{Cuoco:2020ogp}.

Other projects trying to identify glitches exist~\cite{Mukund:2016thr, Cavaglia:2018xjq, Coughlin:2019ref}, many of which rely on the time-frequency representation produced by the Omicron software package~\cite{Robinet:2020lbf}.

\subsection{Gravitational-Wave Searches}
More important for this thesis, there exists a wide variety of \acrshort{ml} based \acrshort{gw} search algorithms. It is currently a very active area of research with several groups around the world providing rapid improvements over initial algorithms. The earliest works based on random forests use data products from other search algorithms or hand crafted features to identify signals~\cite{Baker:2014eba, Kapadia:2017fhb}.

The first proof of principle using deep learning to directly detect \acrshort{bbh} signals in time series data was proposed by George et al. in 2016~\cite{George:2016hay}. They used a 3-layer \acrshort{cnn} to process \SI{1}{\second} of whitened time series data sampled at \SI{8}{\kilo\hertz} from a single detector and classified it into the two categories ``signal'' and ``noise''. Their network was trained on non-spinning \acrshort{bbh} waveforms with masses between \SI{5}{M_\odot} and \SI{75}{M_\odot}. Due to these simplifications, the parameter space consisted only of the two dimensional $m_1-m_2$-plane. For this restricted parameter space they demonstrated the ability of their algorithm to be competitive in signal recovery to matched filtering for high false-alarm probabilities (\acrshort{fap}s). As a difference to \acrshort{far}s, \acrshort{fap}s, in the context of this work, are not derived on long duration data but from individual samples that either do or do not contain a signal. The \acrshort{fap} is then the number of false positives divided by the number of true negatives. Independently, Gabbard et al. developed a similar algorithm~\cite{Gabbard:2017lja} and verified the findings of \cite{George:2016hay}. They also extended it to lower \acrshort{fap}s. Both works, however, were limited by testing the signal recovery only on discrete samples, where each sample consisted of either a well aligned waveform submerged in noise or pure noise.

The first study using deep learning to detect \acrshort{bns} signals was published by Krastev in 2019~\cite{Krastev:2019koe}. He used a network architecture similar to that of \cite{George:2016hay} to process \SI{10}{\second} sampled at \SI{4}{\kilo\hertz} to distinguish pure noise, \acrshort{bbh} signals, and \acrshort{bns} signals. He was able to reproduce the performance on \acrshort{bbh} signals from \cite{George:2016hay, Gabbard:2017lja}, but his method was significantly less sensitive to \acrshort{bns} signals. He also quoted performance figures only as a function of \acrshort{fap} derived on discrete samples rather than in terms of \acrshort{far} derived on continuous data. The study was later extended to cover real detector noise and produce point estimates for the source parameters~\cite{Krastev:2020skk}. In an independent work Sch√§fer et al. proposed a novel \acrshort{nn} architecture tailored toward the detection of \acrshort{bns} signals that allowed processing of \SI{32}{\second} of data~\cite{Schafer:2020kor}. To enable processing so much data, they introduced a multi-rate sampling approach, that reduced the size of the data by a factor of $9$ while preserving all relevant information. This allowed them to be substantially more sensitive than \cite{Krastev:2019koe} to low \acrshort{snr} signals. However, their approach does not generalize well to high \acrshort{snr} signals and is incapable of detecting \acrshort{bbh} signals. They also tested the algorithm on a continuous data set down to a \acrshort{far} of $0.3$ per month, thus providing a direct grounds of comparison to state-of-the-art matched filter search pipelines. It showed that there is a significant performance gap especially for low \acrshort{far}s. Furthermore, they analyzed data from two detectors in a single network. This approach is briefly summarized in chapter \ref{ch:bns} of this thesis. 

The studies discussed so far made use of a \acrshort{cnn} with a few fully connected layers to generate the classification output. This architectural choice requires the use of a sliding window approach, when the networks should be applied to data of duration longer than the input the network was trained on. Gebhard et al. introduced a fully convolutional architecture in \cite{Gebhard:2019ldz}, which allows for the network to be applied to input of arbitrary sizes. It also produces outputs with a higher time resolution than partially convolutional networks. The authors provide criticism of the evaluation procedure used in previous works, which quote performance metrics in terms of \acrshort{fap}s instead of \acrshort{far}s. They then provide extensive studies of their own  approach and test it in terms of \acrshort{far}s. They also investigate the timing accuracy of their network to the location of the merger. Their architecture was picked up and further improved by Wei et al. in \cite{Wei:2020ztw}. This improved architecture was tested on real data from \acrshort{o2} and \acrshort{o3} and is capable of detecting real \acrshort{gw} events in the data at a \acrshort{far} of $2.7$ per day. Both of these networks were also designed to process data from multiple detectors.

Another approach that has started to be explored is building \acrshort{nn}s inspired by matched filtering. Wang et al. create a convolutional layer from a reduced template bank of whitened waveforms and use it to perform a matched filter operation in the time domain~\cite{Wang:2019zaj}. Their network takes an estimate of the \acrshort{psd} into account and uses a \acrshort{cnn} to process the resulting \acrshort{snr} time series. They manage to detect all \acrshort{gw} events from \acrshort{o1} at a high \acrshort{far}. The group also contributed to the mock data challenge discussed in chapter \ref{ch:mlgwsc1}, where their approach is evaluated at low \acrshort{far}s. Yan et al. notice that matched filtering searches are formally equivalent to a particular \acrshort{nn} architecture, that can be hand crafted~\cite{Yan:2021wml}. However, they highlight that matched filtering is not optimal when the signal is not known exactly, as it is not mathematically guaranteed that it minimizes the \acrshort{far} for a given true positive rate. As a consequence, they initialize their network with a given discrete template bank and fine tune it in the hope of finding a better detection criterion. They claim that their approach can consistently outperform matched filtering. However, they test a limited mass range of \SI{40}{M_\odot} to \SI{50}{M_\odot}, where they use up to $10\,000$ templates, use only data from a single detector, and maximize over the raw \acrshort{snr} of all templates. The use of a possibly over-dense template bank, as well as the lack of coincidence and signal consistency tests artificially increase the \acrshort{far} of any detection. Nonetheless, they highlight important deficiencies of matched filtering and their work demonstrates a possible avenue for deep learning to go beyond the capability of existing methods.

The previously discussed algorithms have all made use of time series data. Most \acrshort{ml} advances, on the other hand, originate in computer vision, which processes two dimensional images. For this reason, some studies have looked at a time-frequency representation of the data to make use of such concepts. Wei et al. utilize a ResNet50~\cite{He:2015aaa} pre-trained on image classification data to create early warnings for \acrshort{bns} mergers from spectrograms of the data~\cite{Wei:2020sfz}. The network is fine tuned on injected \acrshort{gw} data and is resilient to glitches. The network is capable of detecting GW170817 \SI{10}{\second} before the merger. Aveiro et al. use the object detection network \acrshort{yolo}v5~\cite{Jocher:2022aaa} to accurately locate \acrshort{gw} mergers in these time-frequency plots. While their work is still at a proof of concept stage, this domain transform highlights an under utilized aspect of machine learning, where rapid development in other fields can improve the capability of \acrshort{gw} detection.

Some works try to utilize the computational efficiency of \acrshort{nn}s along a similar route as Wei et al.~\cite{Wei:2020sfz}. They try to produce early warnings for \acrshort{bns} signals to increase the probability of successful prompt \acrshort{em} follow-up observations. Baltus et al.~\cite{Baltus:2021nme, Baltus:2021emh} use a \acrshort{cnn} similar in nature to the early works by George et al.~\cite{George:2016hay} and Gabbard et al.~\cite{Gabbard:2017lja} to analyze time series data. In the optimal case of strong signals they expect an early warning of up to \SI{100}{\second}. Yu et al.~\cite{Yu:2021vvm} use the detector output as well as auxiliary noise channels to reduce non-linear noise couplings and increase their sensitivity to low-frequency signals. Their network is trained to produce early warnings for \acrshort{bns} and \acrshort{nsbh} signals. They claim similar early warning capabilities as Baltus et al. Chapter \ref{ch:forecasting} of this thesis summarizes a qualitative analysis of early warning capabilities of an existing state-of-the-art matched filtering based search pipeline. Such studies are important on their own but are also imperative as a point of comparison to machine learning algorithms. An advantage of the study presented in chapter \ref{ch:forecasting} over the works by Baltus et al. and Yu et al. is the capability of producing a sky-location estimate.

Besides direct searches, many studies are looking into ways to improve existing search pipelines. They mainly try to achieve this goal by adjusting the ranking statistic. Jadhav et al.~\cite{Jadhav:2020oyt} introduce MLStat, a deep learning algorithm that processes time-frequency representations of the data obtained from a continuous wavelet transform to differentiate between \acrshort{cbc} signals and glitches. The model is a pre-trained InceptionV3~\cite{Szegedy:2016aab} image classifier which is fine tuned on parts of the GravitySpy data set~\cite{Zevin:2016qwy}. They use the output of the \acrshort{cbc} class as a probability to re-weight the coincident ranking statistic and report an increase in the sensitive volume of the PyCBC search~\cite{Usman:2015kfa} by up to $30\%$. Instead of creating a new metric to adjust the ranking statistic, McIsaac et al.~\cite{McIsaac:2022odb} use deep learning to improve the existing $\chi^2$ tests. They train a \acrshort{nn} to optimize hyperparameters of a $\chi^2$ test to improve signal recovery and glitch rejection for high mass signals. They quote an improvement in sensitivity of up to $11\%$ to high total mass signals and are confident that such an automatic tuning of hyperparameters can also be used to improve other signal consistency tests. Choudhary et al.~\cite{Choudhary:2022yje} present a \acrshort{nn} that aims to distinguish \acrshort{cbc} signals from blip glitches. While they do not specify its use in altering the ranking statistic, they claim it to be superior in classification to classical $\chi^2$ tests. They introduce a sine-Gaussian projection, which produces a time-frequency representation of the data and use this as input to their network. The loosely modeled coherent wave burst (\acrshort{cwb}) search~\cite{Klimenko:2005xv, Klimenko:2015ypf} has recently introduced a \acrshort{ml} based enhancement to their pipeline~\cite{Mishra:2021tmu}. The original algorithm produces vetos based on summary statistics generated by the search to reduce the impact of non-Gaussian noise transients. These vetos are a binary decision between noise-like events and signal-like events. The \acrshort{ml} improvement uses a learning algorithm known as XGBoost~\cite{XGBoost} to create an ensemble of decision trees. A weighted average of all ensemble outputs is then processed by a Sigmoid activation to produce a continuous output. As input the decision trees use a subset of $14$ summary statistics generated by the search. By using the Sigmoid output as a modification to the original ranking statistic and by eliminating all other vetos, the authors of \cite{Mishra:2021tmu} find an improvement of up to $26\%$ in sensitivity.

Other applications of \acrshort{ml} to \acrshort{gw} searches exist. Some notable works include the application to \acrshort{cw} searches~\cite{Dreissigacker:2019edy, Dreissigacker:2020xfr, Beheshtipour:2020zhb, Beheshtipour:2020nko}, \acrshort{emri} searches~\cite{Zhang:2022xuq}, and the search for novel signals by anomaly detection~\cite{Morawski:2021kxv, Moreno:2021fvp}. The GWSkyNet project uses public data from alerts of \acrshort{gw} events intended for other astronomers for \acrshort{em} follow-up to distinguish between astrophysical events and noise artifacts~\cite{Cabero:2020eik, Abbott:2021cuf}. The aim of the project is to better inform other astronomers about which alerts are most valuable for follow-up observations.

With this plethora of different search algorithms an objective comparison among different approaches and to state-of-the-art methods is desirable. However, this task is complicated by differing data sets and the usage of different evaluation metrics. For instance, in chapter \ref{ch:bns} we find that our approach is substantially more sensitive to quite \acrshort{bns} systems than the work presented in \cite{Krastev:2019koe} but is still far away from the sensitivity of PyCBC Live, a state-of-the-art low-latency \acrshort{gw} search pipeline. For this reason, chapters \ref{ch:training_strats} and \ref{ch:cnn_coinc}, among other contributions, re-analyze the early work by Gabbard et al.~\cite{Gabbard:2017lja} and compare them to PyCBC~\cite{Usman:2015kfa}, both in the single- and multi-detector case. Chapter \ref{ch:mlgwsc1} describes an attempt of creating a reference data set and evaluation metrics to allow for the important quantification of \acrshort{ml} based search algorithm performance. It is one of the goals of this thesis to create an environment that allows for an objective evaluation of \acrshort{ml} algorithms for \acrshort{cbc} detection to enable targeted development and quick adoption of good methods into production searches.

\subsection{Parameter Estimation}
Another important aspect of \acrshort{gw} data analysis is the estimation of source parameters including error estimates. In the last three years major progress has been made in using \acrshort{ml} to rapidly produce posterior estimates. This is of special interest as traditional parameter estimation can take days or even weeks, depending on the source, to finish analyzing a single event.

As parameter estimation is not the main focus of this thesis, I will only highlight a few of the important works of the recent past. The inital work by Chua et al.~\cite{Chua:2019wwt} is capable of producing posteriors in one or two dimensions by producing histograms or parameters for a Gaussian mixture distribution. Gabbard et al.~\cite{Gabbard:2019rde} use a conditional variational autoencode network to predict the full $15$ dimensional posterior of \acrshort{bbh} directly from the whitened strain data. A variational autoencoder creates a latent representation of the input data in terms of parametrized probability distributions using an encoder. The distribution is then sampled and each sample is processed by a decoder, which in turn produces a second set of parametrized distributions. A single sample is drawn from this second set of distributions for every sample from the latent distributions. In this way a posterior is built up. Green et al.~\cite{Green:2020hst, Green:2020dnx, Dax:2021tsq} use normalizing flows to produce posteriors. The idea of a normalizing flow is to find the transformation from a simple distribution, like a Gaussian distribution, to the target distribution. In their case the target distribution is the \acrshort{gw} posterior for the parameters. They also include knowledge about symmetries of the parameters in their algorithm to simplify the task. All of these algorithms have the advantage that they do not need true posteriors as targets. Instead they learn by sampling the prior distribution during the training process. This means that training data is drawn from the prior and the networks only require the true parameter as label. Chatterjee et al.~\cite{Chatterjee:2022ggk} also use a normalizing flow for rapid sky-location estimation. It can produce accurate estimates of the source location in milliseconds. Other than previous deep learning methods it does so not only for \acrshort{bbh}, but also for \acrshort{bns} and \acrshort{nsbh} sources.

A completely different approach was taken by Williams et al.~\cite{Williams:2021qyt}. They create an algorithm named NESSAI, which is a drop in replacement for the samplers used in many state-of-the-art parameter estimation codes. Their network is also based on a normalizing flow and predicts the contours of iso-likelihood surfaces based on live-points in a nested sampling algorithm. The normalizing flow then allows them to efficiently sample the contour, resulting in an increase in the speed of nested sampling by a factor of $1.4$ over the nested sampler DYNESTY~\cite{Speagle:2020aaa}.
